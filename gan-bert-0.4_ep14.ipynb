{"metadata":{"accelerator":"GPU","colab":{"name":"GANBERT_pytorch croce.ipynb","provenance":[],"collapsed_sections":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"22cde8fbae4e49af993e33f3f2d9a28e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2c671428c4df4355a7a53c71e9bd14ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_44660941ffcc44beae72a1974d458583","IPY_MODEL_afdbd837001c4c74b5ac332ca061ef3a","IPY_MODEL_b489404bf53b4fab9bdb1c0c79a33008"]}},"2c671428c4df4355a7a53c71e9bd14ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"44660941ffcc44beae72a1974d458583":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2bfb43b9e8604cbf8ebfd162267f1b8a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f556917850542da8508f9f839cae9bc"}},"afdbd837001c4c74b5ac332ca061ef3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_28f045edfa48462fa96a94cffd3b143f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14a6abbb244b41f89626a37640c63118"}},"b489404bf53b4fab9bdb1c0c79a33008":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0dccd0ab28c34880be92b35aa05e6ffb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 12.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf127a02cf6647aba4035c5cbfadc378"}},"2bfb43b9e8604cbf8ebfd162267f1b8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f556917850542da8508f9f839cae9bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"28f045edfa48462fa96a94cffd3b143f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"14a6abbb244b41f89626a37640c63118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0dccd0ab28c34880be92b35aa05e6ffb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bf127a02cf6647aba4035c5cbfadc378":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"098f203f1209452a9d4192af92da7057":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ec76cfd2d1da498e9b66885ff1be46b3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b94243bfea7e4441b809b38c7cecd875","IPY_MODEL_0097aa33393342cd99be3dcc30edc5a0","IPY_MODEL_293c4d8660f546f79b43e0dc63250c2f"]}},"ec76cfd2d1da498e9b66885ff1be46b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b94243bfea7e4441b809b38c7cecd875":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9fc9479d78e442db91360de7f45b6d7f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14bf2ab82bfc45bd8a3b93f2ab9aa656"}},"0097aa33393342cd99be3dcc30edc5a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a56008dc6c6b41c3850611cc15fb6ea8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435779157,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435779157,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_504e8c3a107e4e04b51df39e1b3c584e"}},"293c4d8660f546f79b43e0dc63250c2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1f25240f5457445795af70e20e5903f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436M/436M [00:29&lt;00:00, 14.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5110d1cb9a7547f49244113dc5dd8321"}},"9fc9479d78e442db91360de7f45b6d7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14bf2ab82bfc45bd8a3b93f2ab9aa656":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a56008dc6c6b41c3850611cc15fb6ea8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"504e8c3a107e4e04b51df39e1b3c584e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f25240f5457445795af70e20e5903f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5110d1cb9a7547f49244113dc5dd8321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3a13b7869354881ac7b7887e05d56a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4d22913664fb4cdbb38e217d4197601d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_870fe8f58bb24a47b6a98fa0eed0ebf5","IPY_MODEL_193a5a054d6f4a319842820c4c308322","IPY_MODEL_5a478b81997c4263a60d938447232fd9"]}},"4d22913664fb4cdbb38e217d4197601d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"870fe8f58bb24a47b6a98fa0eed0ebf5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5fbebd67d40e470e8a75a4b5b540bbdf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d8b619dff35e4f8cabf586221ad8c962"}},"193a5a054d6f4a319842820c4c308322":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d7bef2816920414f99a5c9cc676ec254","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b465cf9004f549ffa85444bfa16ee4c2"}},"5a478b81997c4263a60d938447232fd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_748d5c1fb00e4d91809003527319a9f6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 102kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d501eb3d36e48128a5232879141b281"}},"5fbebd67d40e470e8a75a4b5b540bbdf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d8b619dff35e4f8cabf586221ad8c962":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d7bef2816920414f99a5c9cc676ec254":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b465cf9004f549ffa85444bfa16ee4c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"748d5c1fb00e4d91809003527319a9f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7d501eb3d36e48128a5232879141b281":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"547d7329b8554b7bb8ae51d61a5e41f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_81b6bafd3fc248afa76cf463f2cb8ab8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bac8f28c84144450b24bbc97fa4860db","IPY_MODEL_30e0829529874db1802f411f72f3d76a","IPY_MODEL_83dd72fbb4204fefb951b3800d73a8d2"]}},"81b6bafd3fc248afa76cf463f2cb8ab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bac8f28c84144450b24bbc97fa4860db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f9390d4fc9b147729f1ef5ea85d03774","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ddf20490dc874352aa7df35f07a60bcc"}},"30e0829529874db1802f411f72f3d76a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c0f0d9a0d4f4440e81e5a6d5a2a3b4ab","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f41b83ccf26c40ed88ca6eaf49e09de5"}},"83dd72fbb4204fefb951b3800d73a8d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be6339f6256d4b579c53ef8b430ecb25","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436k/436k [00:00&lt;00:00, 1.26MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b79fef6d585843c0a4d885edb2d01ebd"}},"f9390d4fc9b147729f1ef5ea85d03774":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ddf20490dc874352aa7df35f07a60bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c0f0d9a0d4f4440e81e5a6d5a2a3b4ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f41b83ccf26c40ed88ca6eaf49e09de5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be6339f6256d4b579c53ef8b430ecb25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b79fef6d585843c0a4d885edb2d01ebd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# STAT 940 Group Project - Group 9\n\nThe code is adapted from the Pytorch implementation of the GAN-BERT model from https://github.com/crux82/ganbert-pytorch, refactored to utilize an alternative dataset and to support a different language.","metadata":{"id":"fUpqAwtN8rTA"}},{"cell_type":"markdown","source":"### Setup","metadata":{"id":"Q0m5KR34gmRH"}},{"cell_type":"code","source":"import torch\nimport io\nimport torch.nn.functional as F\nimport random\nimport numpy as np\nimport time\nimport math\nimport datetime\nimport torch.nn as nn\nimport transformers as tf\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n##Set random values\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\nif torch.cuda.is_available():\n  torch.cuda.manual_seed_all(seed_val)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIqpm34x2rms","outputId":"b0205d19-dff1-4967-d003-990c3c5c8164","execution":{"iopub.status.busy":"2024-04-05T19:46:28.388798Z","iopub.execute_input":"2024-04-05T19:46:28.389785Z","iopub.status.idle":"2024-04-05T19:46:38.203515Z","shell.execute_reply.started":"2024-04-05T19:46:28.389738Z","shell.execute_reply":"2024-04-05T19:46:38.202510Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# If there's a GPU available...\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"id":"LeZgRup520II","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b8d1039-e1e6-4712-e77b-e1e9f1b9fbcb","execution":{"iopub.status.busy":"2024-04-05T19:46:41.554748Z","iopub.execute_input":"2024-04-05T19:46:41.555676Z","iopub.status.idle":"2024-04-05T19:46:41.587196Z","shell.execute_reply.started":"2024-04-05T19:46:41.555641Z","shell.execute_reply":"2024-04-05T19:46:41.586348Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Parameters\n","metadata":{"id":"AU3ns8Ic7I-h"}},{"cell_type":"code","source":"#--------------------------------\n#  Transformer parameters\n#--------------------------------\nmax_seq_length = 64\nbatch_size = 64\n\n#--------------------------------\n#  GAN-BERT specific parameters\n#--------------------------------\n# number of hidden layers in the generator, \n# each of the size of the output space\nnum_hidden_layers_g = 1; \n# number of hidden layers in the discriminator, \n# each of the size of the input space\nnum_hidden_layers_d = 1; \n# size of the generator's input noisy vectors\nnoise_size = 100\n# dropout to be applied to discriminator's input vectors\nout_dropout_rate = 0.2\n\n#--------------------------------\n#  Optimization parameters\n#--------------------------------\nlearning_rate_discriminator = 5e-5\nlearning_rate_generator = 5e-5\nepsilon = 1e-8\nnum_train_epochs = 14\nmulti_gpu = True\n# Scheduler\napply_scheduler = False\nwarmup_proportion = 0.1\n# Print\nprint_each_n_step = 10\n\n#--------------------------------\n#  Adopted Pretrained Tranformer model\n#--------------------------------\n\nmodel_name = \"google-bert/bert-base-chinese\"\n#model_name = \"bert-base-uncased\"\n#model_name = \"roberta-base\"\n#model_name = \"albert-base-v2\"\n#model_name = \"xlm-roberta-base\"\n#model_name = \"amazon/bort\"\n\n\n#--------------------------------\n#  Dataset parameters\n#--------------------------------\nlabel_count=61\nmask_percentage=0.4  # percentage of the training data to be masked","metadata":{"id":"jw0HC_hU3FUy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6ae87fcf-ed0b-4c78-b9aa-7d86d80cb933","execution":{"iopub.status.busy":"2024-04-05T19:46:52.739943Z","iopub.execute_input":"2024-04-05T19:46:52.740866Z","iopub.status.idle":"2024-04-05T19:46:52.747290Z","shell.execute_reply.started":"2024-04-05T19:46:52.740830Z","shell.execute_reply":"2024-04-05T19:46:52.746445Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Load the Pretrained Model","metadata":{"id":"R6Q5jzVioTHb"}},{"cell_type":"code","source":"transformer = tf.AutoModel.from_pretrained(model_name)\ntokenizer = tf.AutoTokenizer.from_pretrained(model_name, use_fast=False)","metadata":{"id":"gxghkkZq3Gbn","outputId":"a4a5afd0-1b6c-4c2d-e3eb-7ced49df4e33","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["22cde8fbae4e49af993e33f3f2d9a28e","2c671428c4df4355a7a53c71e9bd14ee","44660941ffcc44beae72a1974d458583","afdbd837001c4c74b5ac332ca061ef3a","b489404bf53b4fab9bdb1c0c79a33008","2bfb43b9e8604cbf8ebfd162267f1b8a","4f556917850542da8508f9f839cae9bc","28f045edfa48462fa96a94cffd3b143f","14a6abbb244b41f89626a37640c63118","0dccd0ab28c34880be92b35aa05e6ffb","bf127a02cf6647aba4035c5cbfadc378","098f203f1209452a9d4192af92da7057","ec76cfd2d1da498e9b66885ff1be46b3","b94243bfea7e4441b809b38c7cecd875","0097aa33393342cd99be3dcc30edc5a0","293c4d8660f546f79b43e0dc63250c2f","9fc9479d78e442db91360de7f45b6d7f","14bf2ab82bfc45bd8a3b93f2ab9aa656","a56008dc6c6b41c3850611cc15fb6ea8","504e8c3a107e4e04b51df39e1b3c584e","1f25240f5457445795af70e20e5903f9","5110d1cb9a7547f49244113dc5dd8321","d3a13b7869354881ac7b7887e05d56a7","4d22913664fb4cdbb38e217d4197601d","870fe8f58bb24a47b6a98fa0eed0ebf5","193a5a054d6f4a319842820c4c308322","5a478b81997c4263a60d938447232fd9","5fbebd67d40e470e8a75a4b5b540bbdf","d8b619dff35e4f8cabf586221ad8c962","d7bef2816920414f99a5c9cc676ec254","b465cf9004f549ffa85444bfa16ee4c2","748d5c1fb00e4d91809003527319a9f6","7d501eb3d36e48128a5232879141b281","547d7329b8554b7bb8ae51d61a5e41f8","81b6bafd3fc248afa76cf463f2cb8ab8","bac8f28c84144450b24bbc97fa4860db","30e0829529874db1802f411f72f3d76a","83dd72fbb4204fefb951b3800d73a8d2","f9390d4fc9b147729f1ef5ea85d03774","ddf20490dc874352aa7df35f07a60bcc","c0f0d9a0d4f4440e81e5a6d5a2a3b4ab","f41b83ccf26c40ed88ca6eaf49e09de5","be6339f6256d4b579c53ef8b430ecb25","b79fef6d585843c0a4d885edb2d01ebd"]},"execution":{"iopub.status.busy":"2024-04-05T19:46:57.599216Z","iopub.execute_input":"2024-04-05T19:46:57.599589Z","iopub.status.idle":"2024-04-05T19:47:04.525067Z","shell.execute_reply.started":"2024-04-05T19:46:57.599559Z","shell.execute_reply":"2024-04-05T19:47:04.524276Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1974a21139634ac38aa1fdd8d8b27580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a534123e1c1048208105cc68399797d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b52737ece9e4641b5c215d82944d3a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"726924de72b64145b26e3e16205a18ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6aa3a77c74f48da8de5753f3fbc075e"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Prepare Data","metadata":{"id":"K43tOavNqib4"}},{"cell_type":"code","source":"cols_to_remove=[\"id\",\"locale\",\"partition\",\"scenario\",\"annot_utt\",\"slot_method\",\"judgments\",\"worker_id\"]\nnp.object = object\n\ndef add_label_mask(batch,mask=1):\n    batch[\"label_mask\"]=[mask]*len(batch[\"input_ids\"])\n    if mask==0:\n        batch[\"label_id\"]=[60]*len(batch[\"input_ids\"])\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:18.611062Z","iopub.execute_input":"2024-04-05T19:47:18.611793Z","iopub.status.idle":"2024-04-05T19:47:18.617266Z","shell.execute_reply.started":"2024-04-05T19:47:18.611764Z","shell.execute_reply":"2024-04-05T19:47:18.616386Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, Value\n\nd_train_raw=load_dataset(\"AmazonScience/massive\",\"zh-CN\",split=\"train\")\nd_test_raw=load_dataset(\"AmazonScience/massive\",\"zh-CN\",split=\"test\")\n\nsampler=RandomSampler\n\n#------------------------------\n#   Load the train dataset\n#------------------------------\nd_train_t=d_train_raw.map(lambda batch: tokenizer(batch[\"utt\"],truncation=True,padding=\"max_length\",max_length=max_seq_length),batched=True,remove_columns=cols_to_remove).rename_column(\"intent\",\"label_id\")\n\nd_train_split=d_train_t.train_test_split(test_size=mask_percentage,seed=seed_val)\nd_train_nomask=d_train_split[\"train\"].map(add_label_mask,fn_kwargs={\"mask\":1},batched=True)\nd_train_mask=d_train_split[\"test\"].map(add_label_mask,fn_kwargs={\"mask\":0},batched=True)\nd_train=concatenate_datasets([d_train_nomask,d_train_mask])\nprint(d_train)\nd_train=d_train.cast_column(\"label_mask\",Value(dtype=\"bool\")).with_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label_id\", \"label_mask\"], device=device)\n\ntrain_dataloader=DataLoader(d_train, sampler = sampler(d_train), batch_size=batch_size)\n\n#------------------------------\n#   Load the test dataset\n#------------------------------\nd_test=d_test_raw.map(lambda batch: tokenizer(batch[\"utt\"],truncation=True,padding=\"max_length\",max_length=max_seq_length),batched=True,remove_columns=cols_to_remove).map(add_label_mask,fn_kwargs={\"mask\":1},batched=True)\\\n.rename_column(\"intent\",\"label_id\")\nprint(d_test)\nd_test=d_test.cast_column(\"label_mask\",Value(dtype=\"bool\")).with_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label_id\", \"label_mask\"], device=device)\n\ntest_dataloader=DataLoader(d_test,batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:20.811865Z","iopub.execute_input":"2024-04-05T19:47:20.812698Z","iopub.status.idle":"2024-04-05T19:47:43.878001Z","shell.execute_reply.started":"2024-04-05T19:47:20.812662Z","shell.execute_reply":"2024-04-05T19:47:43.877270Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/30.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fb42f82c08245be8103971594bc4ee2"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset massive/zh-CN to /root/.cache/huggingface/datasets/AmazonScience___massive/zh-CN/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/40.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c68ed5bebf44e379c5903a63006718e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset massive downloaded and prepared to /root/.cache/huggingface/datasets/AmazonScience___massive/zh-CN/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b22375971048beac124c578d9e7159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21db9a5aa3f84fee948259174d5696cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18f0e77db814e348b824b5129b1b119"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['label_id', 'utt', 'input_ids', 'token_type_ids', 'attention_mask', 'label_mask'],\n    num_rows: 11514\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c12c2dc60c466ba91542c4cbfe1151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70b440c6fc4455a97854c94403171b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f84cea1c1e44044aff60947f3a7378c"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['label_id', 'utt', 'input_ids', 'token_type_ids', 'attention_mask', 'label_mask'],\n    num_rows: 2974\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27f81b2839074ad5a5f80fd6956dca9b"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Example instance from the dataset","metadata":{}},{"cell_type":"code","source":"dl=DataLoader(d_train,batch_size=batch_size)\nnext(iter(dl))","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:55.482781Z","iopub.execute_input":"2024-04-05T19:47:55.483804Z","iopub.status.idle":"2024-04-05T19:47:55.777133Z","shell.execute_reply.started":"2024-04-05T19:47:55.483760Z","shell.execute_reply":"2024-04-05T19:47:55.776142Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'label_id': tensor([12, 36, 22,  3, 47, 36, 36, 19, 22, 32, 35, 12, 33, 32, 20, 55, 22, 22,\n         23, 50, 13, 34, 58, 59, 20, 33, 12, 42, 45, 40, 40, 19, 45, 49, 25, 56,\n         33, 30,  3, 21, 46, 10, 48, 20, 32, 54, 12, 32, 45, 22, 44, 13,  0, 13,\n         33, 40, 28, 13, 10, 33, 50, 26, 32, 12], device='cuda:0'),\n 'input_ids': tensor([[ 101,  872, 6230,  ...,    0,    0,    0],\n         [ 101, 3173, 4638,  ...,    0,    0,    0],\n         [ 101, 2582,  720,  ...,    0,    0,    0],\n         ...,\n         [ 101, 5314, 2769,  ...,    0,    0,    0],\n         [ 101, 1453,  758,  ...,    0,    0,    0],\n         [ 101,  872, 4500,  ...,    0,    0,    0]], device='cuda:0'),\n 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n 'label_mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n         True, True, True, True, True, True, True, True, True, True, True, True,\n         True, True, True, True, True, True, True, True, True, True, True, True,\n         True, True, True, True, True, True, True, True, True, True, True, True,\n         True, True, True, True, True, True, True, True, True, True, True, True,\n         True, True, True, True], device='cuda:0')}"},"metadata":{}}]},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"id":"fmKL5AD7I4Zg","execution":{"iopub.status.busy":"2024-04-05T19:47:59.099959Z","iopub.execute_input":"2024-04-05T19:47:59.100672Z","iopub.status.idle":"2024-04-05T19:47:59.105773Z","shell.execute_reply.started":"2024-04-05T19:47:59.100638Z","shell.execute_reply":"2024-04-05T19:47:59.104854Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### GAN\nGenerator and Discriminator as in https://www.aclweb.org/anthology/2020.acl-main.191/","metadata":{"id":"6Ihcw3vquaQm"}},{"cell_type":"code","source":"#------------------------------\n#   The Generator as in \n#   https://www.aclweb.org/anthology/2020.acl-main.191/\n#   https://github.com/crux82/ganbert\n#------------------------------\nclass Generator(nn.Module):\n    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n        super(Generator, self).__init__()\n        layers = []\n        hidden_sizes = [noise_size] + hidden_sizes\n        for i in range(len(hidden_sizes)-1):\n            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n\n        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, noise):\n        output_rep = self.layers(noise)\n        return output_rep\n\n#------------------------------\n#   The Discriminator\n#   https://www.aclweb.org/anthology/2020.acl-main.191/\n#   https://github.com/crux82/ganbert\n#------------------------------\nclass Discriminator(nn.Module):\n    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n        super(Discriminator, self).__init__()\n        self.input_dropout = nn.Dropout(p=dropout_rate)\n        layers = []\n        hidden_sizes = [input_size] + hidden_sizes\n        for i in range(len(hidden_sizes)-1):\n            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n\n        self.layers = nn.Sequential(*layers) #per il flatten\n        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, input_rep):\n        input_rep = self.input_dropout(input_rep)\n        last_rep = self.layers(input_rep)\n        logits = self.logit(last_rep)\n        probs = self.softmax(logits)\n        return last_rep, logits, probs","metadata":{"id":"18kY64-n3I6y","execution":{"iopub.status.busy":"2024-04-05T19:48:01.819746Z","iopub.execute_input":"2024-04-05T19:48:01.821107Z","iopub.status.idle":"2024-04-05T19:48:01.833057Z","shell.execute_reply.started":"2024-04-05T19:48:01.821066Z","shell.execute_reply":"2024-04-05T19:48:01.832313Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Instantiate the Discriminator and Generator","metadata":{"id":"Uje9s2zQunFc"}},{"cell_type":"code","source":"# The config file is required to get the dimension of the vector produced by \n# the underlying transformer\nconfig = tf.AutoConfig.from_pretrained(model_name)\nhidden_size = int(config.hidden_size)\n# Define the number and width of hidden layers\nhidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\nhidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n\n#-------------------------------------------------\n#   Instantiate the Generator and Discriminator\n#-------------------------------------------------\ngenerator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\ndiscriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=label_count, dropout_rate=out_dropout_rate)\n\n# Put everything in the GPU if available\nif torch.cuda.is_available():    \n  generator.cuda()\n  discriminator.cuda()\n  transformer.cuda()\n  if multi_gpu:\n    transformer = torch.nn.DataParallel(transformer)\n\n# print(config)","metadata":{"id":"Ylz5rvqE3U2S","execution":{"iopub.status.busy":"2024-04-05T19:48:07.821704Z","iopub.execute_input":"2024-04-05T19:48:07.822060Z","iopub.status.idle":"2024-04-05T19:48:08.008529Z","shell.execute_reply.started":"2024-04-05T19:48:07.822031Z","shell.execute_reply":"2024-04-05T19:48:08.007771Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{"id":"VG3qzp2-usZE"}},{"cell_type":"code","source":"training_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n#models parameters\ntransformer_vars = [i for i in transformer.parameters()]\nd_vars = transformer_vars + [v for v in discriminator.parameters()]\ng_vars = [v for v in generator.parameters()]\n\n#optimizer\ndis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\ngen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n\n#scheduler\nif apply_scheduler:\n  num_train_examples = len(train_examples)\n  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n  num_warmup_steps = int(num_train_steps * warmup_proportion)\n\n  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n                                           num_warmup_steps = num_warmup_steps)\n  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n                                           num_warmup_steps = num_warmup_steps)\n\n# For each epoch...\nfor epoch_i in range(0, num_train_epochs):\n    # ========================================\n    #               Training\n    # ========================================\n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    tr_g_loss = 0\n    tr_d_loss = 0\n\n    # Put the model into training mode.\n    transformer.train() \n    generator.train()\n    discriminator.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every print_each_n_step batches.\n        if step % print_each_n_step == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        b_input_ids = batch[\"input_ids\"]\n        b_input_mask = batch[\"attention_mask\"]\n        b_labels = batch[\"label_id\"]\n        b_label_mask = batch[\"label_mask\"]\n\n        real_batch_size = b_input_ids.shape[0]\n     \n        # Encode real data in the Transformer\n        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n        hidden_states = model_outputs[-1]\n        \n        # Generate fake data that should have the same distribution of the ones\n        # encoded by the transformer. \n        # First noisy input are used in input to the Generator\n        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n        # Gnerate Fake data\n        gen_rep = generator(noise)\n\n        # Generate the output of the Discriminator for real and fake data.\n        # First, we put together the output of the tranformer and the generator\n        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n        # Then, we select the output of the disciminator\n        features, logits, probs = discriminator(disciminator_input)\n\n        # Finally, we separate the discriminator's output for the real and fake\n        # data\n        features_list = torch.split(features, real_batch_size)\n        D_real_features = features_list[0]\n        D_fake_features = features_list[1]\n      \n        logits_list = torch.split(logits, real_batch_size)\n        D_real_logits = logits_list[0]\n        D_fake_logits = logits_list[1]\n        \n        probs_list = torch.split(probs, real_batch_size)\n        D_real_probs = probs_list[0]\n        D_fake_probs = probs_list[1]\n\n        #---------------------------------\n        #  LOSS evaluation\n        #---------------------------------\n        # Generator's LOSS estimation\n        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n        g_loss = g_loss_d + g_feat_reg\n  \n        # Disciminator's LOSS estimation\n        logits = D_real_logits[:,0:-1]\n        log_probs = F.log_softmax(logits, dim=-1)\n        # The discriminator provides an output for labeled and unlabeled real data\n        # so the loss evaluated for unlabeled data is ignored (masked)\n        label2one_hot = torch.nn.functional.one_hot(b_labels, label_count)\n        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n        try:\n            per_example_loss = torch.masked_select(per_example_loss, b_label_mask)\n        except Exception as e:\n            print(per_example_loss.shape, b_label_mask.shape)\n            raise e\n        labeled_example_count = per_example_loss.type(torch.float32).numel()\n\n        # It may be the case that a batch does not contain labeled examples, \n        # so the \"supervised loss\" in this case is not evaluated\n        if labeled_example_count == 0:\n          D_L_Supervised = 0\n        else:\n          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n                 \n        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n\n        #---------------------------------\n        #  OPTIMIZATION\n        #---------------------------------\n        # Avoid gradient accumulation\n        gen_optimizer.zero_grad()\n        dis_optimizer.zero_grad()\n\n        # Calculate weigth updates\n        # retain_graph=True is required since the underlying graph will be deleted after backward\n        g_loss.backward(retain_graph=True)\n        d_loss.backward() \n        \n        # Apply modifications\n        gen_optimizer.step()\n        dis_optimizer.step()\n\n        # A detail log of the individual losses\n        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n        #             g_loss_d, g_feat_reg))\n\n        # Save the losses to print them later\n        tr_g_loss += g_loss.item()\n        tr_d_loss += d_loss.item()\n\n        # Update the learning rate with the scheduler\n        if apply_scheduler:\n          scheduler_d.step()\n          scheduler_g.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #     TEST ON THE EVALUATION DATASET\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our test set.\n    print(\"\")\n    print(\"Running Test...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    transformer.eval() #maybe redundant\n    discriminator.eval()\n    generator.eval()\n\n    # Tracking variables \n    total_test_accuracy = 0\n   \n    total_test_loss = 0\n    nb_test_steps = 0\n\n    all_preds = []\n    all_labels_ids = []\n\n    #loss\n    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n\n    # Evaluate data for one epoch\n    for batch in test_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        b_input_ids = batch[\"input_ids\"]\n        b_input_mask = batch[\"attention_mask\"]\n        b_labels = batch[\"label_id\"]\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n            _, logits, probs = discriminator(hidden_states)\n            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n            filtered_logits = logits[:,0:-1]\n            # Accumulate the test loss.\n            total_test_loss += nll_loss(filtered_logits, b_labels)\n            \n        # Accumulate the predictions and the input labels\n        _, preds = torch.max(filtered_logits, 1)\n        all_preds += preds.detach().cpu()\n        all_labels_ids += b_labels.detach().cpu()\n\n    # Report the final accuracy for this validation run.\n    all_preds = torch.stack(all_preds).numpy()\n    all_labels_ids = torch.stack(all_labels_ids).numpy()\n    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_test_loss = total_test_loss / len(test_dataloader)\n    avg_test_loss = avg_test_loss.item()\n    \n    # Measure how long the validation run took.\n    test_time = format_time(time.time() - t0)\n    \n    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n    print(\"  Test took: {:}\".format(test_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss generator': avg_train_loss_g,\n            'Training Loss discriminator': avg_train_loss_d,\n            'Valid. Loss': avg_test_loss,\n            'Valid. Accur.': test_accuracy,\n            'Training Time': training_time,\n            'Test Time': test_time\n        }\n    )","metadata":{"id":"NhqylHGK3Va4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"726efd06-d8de-4a45-a7bb-f186994a6b2a","execution":{"iopub.status.busy":"2024-04-05T19:48:14.890863Z","iopub.execute_input":"2024-04-05T19:48:14.891258Z","iopub.status.idle":"2024-04-05T20:17:42.430135Z","shell.execute_reply.started":"2024-04-05T19:48:14.891225Z","shell.execute_reply":"2024-04-05T20:17:42.429207Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 14 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"2024-04-05 19:48:21.683427: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-05 19:48:21.683531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-05 19:48:21.978958: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"  Batch    10  of    180.    Elapsed: 0:00:26.\n  Batch    20  of    180.    Elapsed: 0:00:32.\n  Batch    30  of    180.    Elapsed: 0:00:38.\n  Batch    40  of    180.    Elapsed: 0:00:44.\n  Batch    50  of    180.    Elapsed: 0:00:50.\n  Batch    60  of    180.    Elapsed: 0:00:56.\n  Batch    70  of    180.    Elapsed: 0:01:03.\n  Batch    80  of    180.    Elapsed: 0:01:09.\n  Batch    90  of    180.    Elapsed: 0:01:15.\n  Batch   100  of    180.    Elapsed: 0:01:22.\n  Batch   110  of    180.    Elapsed: 0:01:28.\n  Batch   120  of    180.    Elapsed: 0:01:35.\n  Batch   130  of    180.    Elapsed: 0:01:41.\n  Batch   140  of    180.    Elapsed: 0:01:48.\n  Batch   150  of    180.    Elapsed: 0:01:54.\n  Batch   160  of    180.    Elapsed: 0:02:01.\n  Batch   170  of    180.    Elapsed: 0:02:08.\n\n  Average training loss generetor: 0.644\n  Average training loss discriminator: 3.809\n  Training epcoh took: 0:02:14\n\nRunning Test...\n  Accuracy: 0.730\n  Test Loss: 1.335\n  Test took: 0:00:08\n\n======== Epoch 2 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:32.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:45.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:58.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:11.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.743\n  Average training loss discriminator: 1.748\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.820\n  Test Loss: 0.801\n  Test took: 0:00:08\n\n======== Epoch 3 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:20.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:33.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:45.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:58.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:11.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.727\n  Average training loss discriminator: 1.279\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.836\n  Test Loss: 0.699\n  Test took: 0:00:08\n\n======== Epoch 4 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:33.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:46.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:59.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:12.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.721\n  Average training loss discriminator: 1.071\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.846\n  Test Loss: 0.691\n  Test took: 0:00:08\n\n======== Epoch 5 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:20.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:33.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:46.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:59.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:12.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.716\n  Average training loss discriminator: 0.995\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.841\n  Test Loss: 0.707\n  Test took: 0:00:08\n\n======== Epoch 6 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:32.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:45.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:58.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:11.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:24.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:37.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.714\n  Average training loss discriminator: 0.905\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.835\n  Test Loss: 0.756\n  Test took: 0:00:08\n\n======== Epoch 7 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:20.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:33.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:46.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:58.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:12.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.711\n  Average training loss discriminator: 0.859\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.834\n  Test Loss: 0.802\n  Test took: 0:00:08\n\n======== Epoch 8 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:33.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:46.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:59.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:12.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.711\n  Average training loss discriminator: 0.836\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.840\n  Test Loss: 0.804\n  Test took: 0:00:08\n\n======== Epoch 9 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:32.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:45.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:58.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:11.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:24.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:37.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:50.\n\n  Average training loss generetor: 0.711\n  Average training loss discriminator: 0.804\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.839\n  Test Loss: 0.822\n  Test took: 0:00:08\n\n======== Epoch 10 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:20.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:33.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:46.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:59.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:12.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.710\n  Average training loss discriminator: 0.817\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.839\n  Test Loss: 0.820\n  Test took: 0:00:08\n\n======== Epoch 11 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:20.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:32.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:45.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:58.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:11.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:24.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:51.\n\n  Average training loss generetor: 0.710\n  Average training loss discriminator: 0.803\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.839\n  Test Loss: 0.858\n  Test took: 0:00:08\n\n======== Epoch 12 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:32.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:45.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:59.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:12.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:38.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:50.\n\n  Average training loss generetor: 0.710\n  Average training loss discriminator: 0.776\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.832\n  Test Loss: 0.896\n  Test took: 0:00:08\n\n======== Epoch 13 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:33.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:46.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:59.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:12.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:25.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:37.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:50.\n\n  Average training loss generetor: 0.709\n  Average training loss discriminator: 0.793\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.839\n  Test Loss: 0.879\n  Test took: 0:00:08\n\n======== Epoch 14 / 14 ========\nTraining...\n  Batch    10  of    180.    Elapsed: 0:00:07.\n  Batch    20  of    180.    Elapsed: 0:00:13.\n  Batch    30  of    180.    Elapsed: 0:00:19.\n  Batch    40  of    180.    Elapsed: 0:00:26.\n  Batch    50  of    180.    Elapsed: 0:00:32.\n  Batch    60  of    180.    Elapsed: 0:00:39.\n  Batch    70  of    180.    Elapsed: 0:00:45.\n  Batch    80  of    180.    Elapsed: 0:00:52.\n  Batch    90  of    180.    Elapsed: 0:00:58.\n  Batch   100  of    180.    Elapsed: 0:01:05.\n  Batch   110  of    180.    Elapsed: 0:01:11.\n  Batch   120  of    180.    Elapsed: 0:01:18.\n  Batch   130  of    180.    Elapsed: 0:01:24.\n  Batch   140  of    180.    Elapsed: 0:01:31.\n  Batch   150  of    180.    Elapsed: 0:01:37.\n  Batch   160  of    180.    Elapsed: 0:01:44.\n  Batch   170  of    180.    Elapsed: 0:01:50.\n\n  Average training loss generetor: 0.710\n  Average training loss discriminator: 0.802\n  Training epcoh took: 0:01:57\n\nRunning Test...\n  Accuracy: 0.835\n  Test Loss: 0.864\n  Test took: 0:00:08\n","output_type":"stream"}]},{"cell_type":"code","source":"for stat in training_stats:\n  print(stat)\n\nprint(\"\\nTraining complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"id":"dDm9NProRB4c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ffebcf1-6b39-4442-88c6-5f72a58a3722","execution":{"iopub.status.busy":"2024-04-05T20:22:33.849303Z","iopub.execute_input":"2024-04-05T20:22:33.850137Z","iopub.status.idle":"2024-04-05T20:22:33.855777Z","shell.execute_reply.started":"2024-04-05T20:22:33.850104Z","shell.execute_reply":"2024-04-05T20:22:33.854838Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"{'epoch': 1, 'Training Loss generator': 0.6440399405856927, 'Training Loss discriminator': 3.808991653389401, 'Valid. Loss': 1.3345180749893188, 'Valid. Accur.': 0.7303295225285811, 'Training Time': '0:02:14', 'Test Time': '0:00:08'}\n{'epoch': 2, 'Training Loss generator': 0.74280021654235, 'Training Loss discriminator': 1.7475153790579903, 'Valid. Loss': 0.8013620376586914, 'Valid. Accur.': 0.8197713517148622, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 3, 'Training Loss generator': 0.7273219191365772, 'Training Loss discriminator': 1.2794707083039814, 'Valid. Loss': 0.6994433999061584, 'Valid. Accur.': 0.83591123066577, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 4, 'Training Loss generator': 0.720562281873491, 'Training Loss discriminator': 1.0708700829082065, 'Valid. Loss': 0.690639853477478, 'Valid. Accur.': 0.8463349024882313, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 5, 'Training Loss generator': 0.7157066603501637, 'Training Loss discriminator': 0.9951630767848757, 'Valid. Loss': 0.7074040770530701, 'Valid. Accur.': 0.8409549428379287, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 6, 'Training Loss generator': 0.713693724738227, 'Training Loss discriminator': 0.9052517804834578, 'Valid. Loss': 0.7560276985168457, 'Valid. Accur.': 0.8349024882313383, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 7, 'Training Loss generator': 0.7112980657153659, 'Training Loss discriminator': 0.8594334748056199, 'Valid. Loss': 0.8022654056549072, 'Valid. Accur.': 0.8342299932750504, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 8, 'Training Loss generator': 0.7110424154334598, 'Training Loss discriminator': 0.8355618251694573, 'Valid. Loss': 0.8044414520263672, 'Valid. Accur.': 0.8402824478816409, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 9, 'Training Loss generator': 0.7110700190067292, 'Training Loss discriminator': 0.8035638514492247, 'Valid. Loss': 0.8223673105239868, 'Valid. Accur.': 0.8386012104909213, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 10, 'Training Loss generator': 0.7103305707375208, 'Training Loss discriminator': 0.8168849365578758, 'Valid. Loss': 0.8198307156562805, 'Valid. Accur.': 0.8392737054472091, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 11, 'Training Loss generator': 0.7101988901694616, 'Training Loss discriminator': 0.8030233260658052, 'Valid. Loss': 0.8576745986938477, 'Valid. Accur.': 0.8386012104909213, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 12, 'Training Loss generator': 0.7101823492182626, 'Training Loss discriminator': 0.7755624804231855, 'Valid. Loss': 0.8962520360946655, 'Valid. Accur.': 0.8315400134498991, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 13, 'Training Loss generator': 0.7088097910086314, 'Training Loss discriminator': 0.7926985058519576, 'Valid. Loss': 0.8785077929496765, 'Valid. Accur.': 0.8392737054472091, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n{'epoch': 14, 'Training Loss generator': 0.7099037551217608, 'Training Loss discriminator': 0.8016160597403844, 'Valid. Loss': 0.8635110259056091, 'Valid. Accur.': 0.8345662407531943, 'Training Time': '0:01:57', 'Test Time': '0:00:08'}\n\nTraining complete!\nTotal training took 0:34:19 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}